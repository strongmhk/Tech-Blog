<p>현재 위치 인증 기반 알람 앱 “눈 떠!”의 v1 운영을 잠시 멈추고, 유저 리서치를 기반으로 기능을 재정비 중이다.
앱을 운영하면서 배포에 대해서 많은 고민을 했는데, 그 내용을 글로 남겨보려한다.</p>
<h2 id="1-배포-중-발생한-30초의-공백">1. 배포 중 발생한 30초의 공백</h2>
<p>이전에 개발 단계에서 구축해둔 CI/CD 파이프라인을 통해 배포가 진행되고 있었다.</p>
<p>그러던 중 앱 개발자로부터 연락이 왔다.</p>
<pre><code>👩‍💻 앱 개발자:  “혹시 지금 개발 서버 내려가 있나요?”</code></pre><pre><code>🧑‍💻 나: “네 지금 배포중이라 잠시 사용 불가능 할 거에요”</code></pre><p>그 이유는 배포하는 과정에서 아래의 그림과 같이 먼저 기존 버전을 종료시키고 새로운 버전을 실행시켰기 때문에 종료와 실행 사이의 간격인 약 30~40초 만큼의 중단 시간이 발생했다.</p>
<p>TMI지만 개발 단계에서는 퇴근 후 개발 할 시간이 넉넉하지 않았기에 (하루 평균 약 3시간, 매일 새벽 2시까지 개발하다가 오전 6시에 기상하는 루틴이었다.. ) 개발 서버에 무중단 배포를 적용하는 것은 뒤로 미뤄뒀었다.(운영 서버에는 필수지만 개발 서버에는 반드시 필요한 것은 아니었기에)</p>
<p>하지만 운영 서버는 실제 사용자가 사용하는 것이니 무중단 배포 전략을 반드시 적용해야겠다는 생각을 했다.</p>
<p align="center" style="color: gray;">
  <!-- 마진은 위아래만 조절하는 것이 정신건강에 좋을 듯 하다. 이미지가 커지면 깨지는 경우가 있는 듯 하다.-->
  <img alt="factorio thumbnail" src="https://velog.velcdn.com/images/alsgudtkwjs/post/66759664-03fc-4174-88ba-f07a752faca0/image.png" />
  배포 중 30~40초 간의 공백이 발생
</p> 

<br />
<br />

<h2 id="2-무중단-배포란">2. 무중단 배포란?</h2>
<p>말 그대로 서비스를 중단하지 않고 새로운 버전을 배포하는 개념이다.</p>
<p>무중단 배포를 도입하지 않은 일반적인 배포 방식은 다음과 같다.</p>
<ol>
<li>새 버전의 Docker 이미지 Pull</li>
<li>기존 컨테이너 Stop &amp; Remove</li>
<li>새 버전 컨테이너 Run</li>
</ol>
<p>이 과정에서 서비스가 중단되는 틈이 생긴다. 무중단 배포는 로드밸런서나 프록시 서버를 활용해 두 버전의 서버를 공존시키거나 트래픽을 조절하여 이 다운타임을 없애는 해결책이다. </p>
<p>대표적으로 세 가지 전략이 주로 사용된다.</p>
<br />
<br />

<h2 id="3-무중단-배포-전략-3가지">3. 무중단 배포 전략 3가지</h2>
<h3 id="3-1-rolling-배포">3-1. Rolling 배포</h3>
<p>사용중인 인스턴스들에 새 버전의 서비스를 점진적으로 배포하는 방식이다. 즉, 서비스중인 인스턴스 하나를 로드밸런스에서 라우팅하지 않도록 한 뒤, 새로운 버전의 서비스를 적용하여 다시 라우팅하도록 등록한다. 이를 반복하며 모든 인스턴스에 새 버전의 서비스를 배포한다.</p>
<p>이 방식에는 다음과 같은 장/단점이 있다.</p>
<p align="center" style="color: gray;">
  <!-- 마진은 위아래만 조절하는 것이 정신건강에 좋을 듯 하다. 이미지가 커지면 깨지는 경우가 있는 듯 하다.-->
  <img alt="factorio thumbnail" src="https://velog.velcdn.com/images/alsgudtkwjs/post/b6277078-7096-4c1f-b3e7-e6c53c29f76f/image.png" />
  Rolling 배포
</p> 

<br />

<p><strong>장점</strong></p>
<ol>
<li>구성된 자원을 그대로 유지한 채로 무중단 배포가 가능하므로 관리가 편하다.</li>
<li>인스턴스마다 차례로 배포를 진행하므로, 새 버전의 인스턴스에서 어떤 이슈가 발생했을 경우 쉽게 롤백이 가능하다 </li>
</ol>
<p><strong>단점</strong></p>
<ol>
<li>새 버전 배포중에는 서비스중인 인스턴스 수가 감소하므로 동작중인 인스턴스에 트래픽이 몰릴 수 있다.(만약 롤링 업데이트 과정에 새로운 버전 인스턴스가 1개만 존재 할 경우 트래픽이 몰리면 과부하가 발생할 수 있다.)</li>
<li>또한 진행중엔 기존버전과 새로운 버전이 함께 존재하므로 호환성 문제가 생기기도 한다.</li>
</ol>
<p>따라서 새 배포에 큰 변화가 없거나, 이미 충분히 테스트한 기능일 경우 사용해야하는 점진적 롤백 방식이다.</p>
<br />

<h3 id="3-2-blue-green-배포">3-2. Blue-Green 배포</h3>
<p>Blue라는 기본버전과 Green이라는 새로운 버전을 따로 운용하는 방식이다. Green에 해당하는 인스턴스들에 신버전을 배포하고, 로드밸런서가 모두 Green의 인스턴스를 바라보도록 전환한다.</p>
<p align="center" style="color: gray;">
  <!-- 마진은 위아래만 조절하는 것이 정신건강에 좋을 듯 하다. 이미지가 커지면 깨지는 경우가 있는 듯 하다.-->
  <img alt="factorio thumbnail" src="https://velog.velcdn.com/images/alsgudtkwjs/post/25fa70cb-36a0-46d0-a7fd-c31c6b6c41eb/image.png" />
  Blue-Green 배포
</p> 


<p><strong>장점</strong></p>
<ol>
<li>구버전의 인스턴스가 남아있어 문제시 빠른 롤백이 가능하다.</li>
<li>구 버전과 동일한 환경에서 신 버전 인스턴스를 구성하기 때문에 실제 서비스 환경에서 신 버전을 미리 테스트 할 수 있다.</li>
<li>배포가 완료된 후 남아있는 구 버전 환경을 다음 배포에 재사용 할 수 있다.</li>
<li>신 버전 배포가 진행되는 동안 서버 과부하가 일어날 확률이 적다.</li>
</ol>
<p><strong>단점</strong></p>
<ol>
<li>Blue영역과 Green영역이 따로따로 존재하는것이므로 순간적으로 시스템 자원이 두배 필요해 비용이 더 많이 필요하다는 단점이 있다.</li>
</ol>
<p>따라서 시스템 자원이 충분한 경우에 사용해야한다. 또한 빠르고 안전한 롤백이 필요한 경우에도 유용하다.</p>
<br />

<h3 id="3-3-canary-배포">3-3. Canary 배포</h3>
<p>잠재적 문제 상황을 미리 발견할 수 있는 배포 전략으로, 지정한 서버나 특정 유저에게만 배포했다가 오류가 없다고 판단되면 전체 시스템에 단계적으로 배포하는 방식을 말한다. 즉, 트래픽의 일부만 신버전을 사용하게 한 뒤 오류여부를 확인하는 것이다.</p>
<p align="center" style="color: gray;">
  <!-- 마진은 위아래만 조절하는 것이 정신건강에 좋을 듯 하다. 이미지가 커지면 깨지는 경우가 있는 듯 하다.-->
  <img alt="factorio thumbnail" src="https://velog.velcdn.com/images/alsgudtkwjs/post/80e9d9cc-6bfc-49d2-ad41-37f83a074e70/image.png" />
  Canary 배포
</p> 



<p><strong>장점</strong></p>
<ol>
<li>Blue-Green 배포처럼 신규 버전 배포 전에 실제 운영 환경에서 미리 테스트할 수 있다.(A/B 테스트가 가능하다.)</li>
<li>카나리 배포는 단계적인 전환 방식을 통해 부정적 영향을 최소화하고 상황에 따라 트래픽 양을 늘리거나 롤백할 수 있다.</li>
</ol>
<p><strong>단점</strong></p>
<ol>
<li>네트워크 트래픽 제어에 부담이 될 수 있다는 단점이 존재한다.</li>
</ol>
<p>따라서 소수의 사용자에게만 영향을 끼치는 것이므로 안정성이 중요한 금융시스템 등에서 사용한다. 새로운 기능 혹은 실험적 기능을 도입할때 이용함으로써 문제상황을 초기단계에 발견할수도 있다.</p>
<br />
<br />
<br />

<h2 id="4-그래서-어떤-방법이-우리-서비스에-적합할까">4. 그래서 어떤 방법이 우리 서비스에 적합할까?</h2>
<p><strong>결론부터 말하자면 Blue-Green 배포 전략이다.</strong></p>
<p>현재 “눈 떠!” 서비스는 MAU 약 100명 규모로 단일 인스턴스로도 트래픽 처리가 충분하다. 하지만 신규 기능 개발로 인한 잦은 배포와, 문제 발생 시 <strong>신속한 롤백</strong>이 무엇보다 중요한 상황이었다.</p>
<p>자원 효율성보다는 <strong>안정적인 배포와 빠른 원상복구</strong>가 최우선이었기에, 배포 복잡도는 낮추면서 확실한 전환이 가능한 Blue-Green 방식을 채택했다.</p>
<br />
<br />


<h2 id="5-무중단-배포-적용하기">5. 무중단 배포 적용하기</h2>
<h3 id="5-1-전체-프로세스">5-1. 전체 프로세스</h3>
<p>개발 서버는 빠른 구축이 중요했기 때문에 <strong>GitHub Actions</strong>를 사용했다.
반면 운영 서버는 보안(IP 화이트리스트)과 인프라 관리의 독립성을 고려해 <strong>Jenkins</strong>를 선택했다.
운영 환경의 전체 배포 흐름은 다음과 같다.</p>
<ol>
<li><p><strong>Code Push</strong></p>
<p> <code>main</code> 브랜치에 코드가 병합된다.</p>
</li>
<li><p><strong>Jenkins CI</strong></p>
<p> Jenkins가 Poll SCM 방식으로 변경 사항을 감지한 뒤,
 애플리케이션 빌드 → Docker 이미지 빌드 및 Push를 수행한다.</p>
</li>
<li><p><strong>Deploy</strong></p>
<p> 빌드가 완료되면 Jenkins가 운영 서버(WAS)에 SSH로 접속해 배포 스크립트를 실행한다.</p>
</li>
<li><p><strong>Blue/Green 전환</strong></p>
<p> Nginx가 트래픽을 관리하며, 신규 컨테이너(Green)를 기동 → 헬스 체크 → 트래픽 전환을 수행한다.</p>
</li>
</ol>
<br />
<br />

<h3 id="5-2-핵심-프로세스-무엇이-달라졌을까">5-2. 핵심 프로세스: 무엇이 달라졌을까?</h3>
<p>기존 배포 방식은 다음과 같았다.</p>
<blockquote>
<p>기존 버전 종료 → 신규 버전 실행</p>
</blockquote>
<p>이 방식에서는 <strong>기존 컨테이너가 내려간 뒤 신규 컨테이너가 올라오기까지 약 30초 이상의 공백 시간</strong>이 발생했다. 즉, 그 시간 동안은 실제로 요청을 처리할 서버가 존재하지 않았다.
개선된 배포 방식의 핵심은 <strong>순서</strong>다.</p>
<blockquote>
<p><strong>새 버전 실행(Up)</strong> → <strong>정상 여부 확인(Health Check)</strong> → <strong>트래픽 전환(Reload)</strong> → <strong>구버전 종료(Down)</strong></p>
</blockquote>
<p>이 순서를 통해, <strong>신규 버전이 완전히 준비되기 전까지는 기존 버전이 계속 트래픽을 처리하도록 보장</strong>한다. 아래는 개선 전/후의 배포 흐름을 비교한 이미지다.</p>
<p align="center" style="color: gray;">
  <!-- 마진은 위아래만 조절하는 것이 정신건강에 좋을 듯 하다. 이미지가 커지면 깨지는 경우가 있는 듯 하다.-->
  <img alt="factorio thumbnail" src="https://velog.velcdn.com/images/alsgudtkwjs/post/3a5c7753-17d5-44c4-a76a-f5d6073accbd/image.png" />
  개선 전 : 기존 컨테이너 종료 → 공백 발생 
</p> 

<p align="center" style="color: gray;">
  <!-- 마진은 위아래만 조절하는 것이 정신건강에 좋을 듯 하다. 이미지가 커지면 깨지는 경우가 있는 듯 하다.-->
  <img alt="factorio thumbnail" src="https://velog.velcdn.com/images/alsgudtkwjs/post/490fdd00-f8d4-49bb-b343-cc7146f2f66b/image.png" />
  개선 후 : 신규 컨테이너 준비 완료 후 트래픽 전환
</p>

<br />


<p>아래는 실제로 사용 중인 배포 스크립트의 핵심 부분이다.</p>
<p>주요 흐름은 다음과 같다.</p>
<ol>
<li>신규 컨테이너가 <strong>정상 상태임이 확인되기 전까지 트래픽은 절대 전환되지 않는다</strong></li>
<li>헬스 체크 실패 시 스크립트는 즉시 종료되며, <strong>기존 서비스는 그대로 유지된다</strong></li>
<li>잘못된 버전이 사용자 트래픽을 받는 상황을 원천적으로 차단한다</li>
</ol>
<pre><code class="language-bash">#!/bin/bash
# ... (변수 설정 생략)

# 1. 쉬고 있는(IDLE) 그룹에 새 버전 컨테이너 실행
echo &quot;### Starting new containers for ${IDLE_GROUP_COLOR} group&quot;
docker compose -p nuntteo-${IDLE_GROUP_COLOR} -f ${IDLE_COMPOSE_FILE} pull
docker compose -p nuntteo-${IDLE_GROUP_COLOR} -f ${IDLE_COMPOSE_FILE} up -d

# 2. 새 컨테이너 헬스 체크 (5초 간격, 최대 30회)
for PORT in &quot;${IDLE_PORTS[@]}&quot;; do
  echo &quot;### Health checking port ${PORT}...&quot;
  for i in {1..30}; do
    # Actuator Health Endpoint 호출
    RESPONSE_CODE=$(curl -s -o /dev/null -w &quot;%{http_code}&quot; http://127.0.0.1:${PORT}/actuator/health)

    if [ $RESPONSE_CODE -eq 200 ]; then
      echo &quot;### Port ${PORT} is UP!&quot;
      break
    fi

    if [ $i -eq 30 ]; then
      echo &quot;### Health check failed. Aborting deployment.&quot;
      # 실패 시 스크립트 종료 (기존 서비스 유지)
      exit 1
    fi
    sleep 5
  done
done

# 3. Nginx 트래픽 전환
echo &quot;### Switching Nginx traffic to ${IDLE_GROUP_COLOR}&quot;
sudo sh -c &quot;echo 'set \$service_url http://${IDLE_GROUP_COLOR}_backend;' &gt; ${NGINX_CONF}&quot;
sudo nginx -s reload

# 4. 이전 버전 컨테이너 종료
echo &quot;### Stopping old containers for ${CURRENT_GROUP_COLOR} group&quot;
docker compose -p nuntteo-${CURRENT_GROUP_COLOR} -f ${CURRENT_COMPOSE_FILE} down</code></pre>
<br />
<br />

<h3 id="5-3-spring-boot-actuator-그대로-쓰면-오히려-위험할-수-있다">5-3. Spring Boot Actuator, 그대로 쓰면 오히려 위험할 수 있다</h3>
<p><a href="https://toss.tech/article/how-to-work-health-check-in-spring-boot-actuator">https://toss.tech/article/how-to-work-health-check-in-spring-boot-actuator</a></p>
<p>헬스 체크를 위해 <code>Spring Boot Actuator</code>의 <code>/actuator/health</code> 엔드포인트를 사용했다.</p>
<p>단순히 “애플리케이션이 살아 있는지”만 확인해줄 것이라 생각했지만, 위의 레퍼런스에서 동작 방식을 자세히 살펴보니 주의가 필요하다는 것을 알게 됐다.</p>
<p>Actuator의 <code>/actuator/health</code>는 다음을 함께 판단한다.</p>
<ul>
<li>애플리케이션 프로세스 상태</li>
<li>DB, Redis 등 <strong>외부 의존성의 상태</strong></li>
</ul>
<p>Spring Boot는 classpath에 포함된 라이브러리를 기준으로 관련 <code>HealthIndicator</code>를 <strong>자동으로 등록</strong>하기 때문이다.</p>
<p>문제는 이 동작이 <strong>운영 환경에서는 오히려 장애를 유발할 수 있다는 점</strong>이다.</p>
<p>예를 들어, 로그용 DB나 일시적으로 연결이 끊긴 Redis와 같이 <strong>서비스 핵심 기능과 직접적인 관련이 없는 요소</strong>가 DOWN 상태일 경우에도 애플리케이션 전체 상태를 <code>DOWN</code>으로 판단할 수 있다.</p>
<p>로드밸런서(Nginx)는 이를 장애로 인식해 <strong>정상 동작 중인 인스턴스의 트래픽까지 차단</strong>해버릴 수 있다.</p>
<p><img alt="" src="https://velog.velcdn.com/images/alsgudtkwjs/post/3a5ec6a9-ff14-4ebd-afbe-191483ddc51f/image.png" /></p>
<p>이를 해결하기 위해서는</p>
<ul>
<li>/actuator/health 대신 직접 구현한 헬스 체크 엔드포인트를 사용 (ex : <code>/api/health</code> )</li>
<li>필요없는 Health Indicator를 비활성화(<code>management.health.db.enabled=false</code>) or 커스터마이징</li>
</ul>
<p>하는 방법이 있다.</p>
<br />

<p>나의 경우, 헬스 체크의 목적을 “모든 외부 의존성이 완벽한가?”보다는 <strong>“이 인스턴스를 지금 제거해도 되는가?”</strong>로 정의했다.</p>
<p>따라서 Redis, DiskSpace처럼 <strong>외부 환경이나 일시적인 리소스 상태에 영향을 받는 항목은 비활성화</strong>했다.</p>
<pre><code class="language-java">management.health.redis.enabled=false
management.health.diskspace.enabled=false</code></pre>
<p>이 설정을 통해 일시적인 Redis 장애나 디스크 사용량 변동이 곧바로 서비스 장애로 확대되는 상황을 예방할 수 있었다.</p>
<p>정리하자면,</p>
<p>Actuator Health Check는 “모든 것이 완벽한가?”를 판단하는 도구가 아니라 <strong>“이 인스턴스를 지금 내려도 되는가?”를 판단하는 기준</strong>으로 사용해야 한다.</p>
<br />
<br />

<h2 id="6-적용-효과">6. 적용 효과</h2>
<p>무중단 배포 적용 후 얻게 된 효과는 명확했다.</p>
<p><strong>첫째, 다운타임 0초를 달성했다.</strong>
기존에는 배포 때마다 30초~1분가량 서비스가 멈췄지만, 이제는 사용자가 앱을 사용하는 도중에 배포가 진행되어도 전혀 인지하지 못한다. 사용자 경험(UX) 측면에서 큰 개선을 이뤘다.</p>
<p><strong>둘째, 개발자의 심리적 안정감을 확보했다.</strong>
사실 이게 가장 크다. 예전에는 &quot;혹시 서버 내려갔나요?&quot;라는 말을 들을까 봐, 혹은 사용자가 적은 새벽 시간에만 조심스럽게 배포해야 했다. 하지만 이제는 대낮에도 이슈가 생기면 즉시 Hotfix를 배포할 수 있는 자신감이 생겼다. 헬스 체크 실패 시 Nginx가 트래픽을 전환하지 않고 배포를 중단하므로, 잘못된 코드가 배포되어 서버가 터지는 상황을 막아주는 안전장치 역할도 톡톡히 하고 있다.</p>
<br />
<br />

<h2 id="7-그럼-이제-배포-파이프라인은-문제가-없이-완벽한-상태일까">7. 그럼 이제 배포 파이프라인은 문제가 없이 완벽한 상태일까?</h2>
<p>배포 파이프라인이 많이 개선되었지만, 아직 완벽하지는 않다. 추후 다음과 같은 부분들을 보완해 나갈 예정이다.</p>
<ul>
<li><p><strong>Graceful Shutdown (우아한 종료):</strong> 현재는 <code>docker compose down</code>으로 기존 컨테이너를 종료한다. 만약 기존 컨테이너가 긴 작업을 처리 중이었다면 강제 종료되면서 트랜잭션이 끊길 위험이 있다. 
Spring Boot의 <code>server.shutdown=graceful</code> 옵션과 Nginx 설정을 통해 처리 중인 요청을 끝내고 종료되도록 개선이 필요하다.</p>
</li>
<li><p><strong>DB 스키마 변경 대응:</strong> Blue/Green 배포의 고질적인 문제는 DB 스키마 변경이 있는 배포다. 구버전과 신버전이 동일한 DB를 바라보기 때문에, 하위 호환성이 없는 변경이 일어나면 구버전에서 에러가 발생할 수 있다. 이를 위해 DB 마이그레이션 전략을 더 정교하게 다듬어야 한다.</p>
</li>
<li><p><strong>CD 도구 도입:</strong> 현재는 쉘 스크립트로 배포 로직을 제어하고 있지만, 프로젝트가 커질수록 스크립트 관리가 어려워질 수 있다. 추후에는 전문적인 CD 도구 도입을 고려해 볼 만하다.</p>
</li>
</ul>
<br />
<br />

<h2 id="8-레퍼런스">8. 레퍼런스</h2>
<ul>
<li><p><a href="https://steam-egg.tistory.com/42">https://steam-egg.tistory.com/42</a></p>
</li>
<li><p><a href="https://3juhwan.tistory.com/47">https://3juhwan.tistory.com/47</a></p>
</li>
<li><p><a href="https://loosie.tistory.com/781">https://loosie.tistory.com/781</a></p>
</li>
<li><p><a href="https://velog.io/@dongvelop/Springboot-Graceful-Shutdown">https://velog.io/@dongvelop/Springboot-Graceful-Shutdown</a></p>
</li>
</ul>
